train:
  wandb: True
  device: ~
  local_rank: ~
  exp_name: UCF_256rs_16f_fps32_stacktrm2_lr5e-5

  deepspeed_config: config/deepspeed/deepspeed_base_fp16_bs8_lr5e-5.json
  num_train_steps: 1000000
  num_valid_steps: 2500
  save_ckpt_steps: 5000
  gamma: cosine
  convert_loss_to_fp32: True

generation:
  batch_size: 1
  topk: -1
  samples: 1
  timesteps: 8
  temperature: 0.3
  show_process: True

tokenizer:
  type: shareCB # [default, shareCB]
  num_frames: 16  # number of frames encoded by mocovqvae

vqvae:
  config_file: MoCoVQVAE/config/mocovqvae_wcd_sCB/UCF101/MoCoVQVAEwCD_im256_16frames_id4.yaml
  checkpoint: experiments/exp_vqvae/MoCoVQVAEwCDsCB_UCF_im256_16frames_id4_2022-06-29-13-08-50/MoCoVQVAE_wCD_shareCB_iter250000.pth

dataset:
  name: UCF101
  cname: CustomUnconditionalDataset
  pin_memory: True
  shuffle: False
  num_worker: 4
  train:
    tokens_dir: datasets/UCF101/tokens/train/FPS32
  valid:
    tokens_dir: datasets/UCF101/tokens/valid/FPS32

model:
  name: StackTRM2_shareCB

  transformer: stack
  num_layer: 16
  mo_num_layer: 8
  embed_dim: 758
  hidden_dim: 1024
  immediate_hidden_dim: 2048
  encoder_opt:
    num_head: 8

  bg_token_length: 1024 # H-32, W-32
  id_token_length: 256  # H-16, W-16
  mo_token_length: 1024 # T-16, H-8, W-8
  max_input_length: 1300 # max(bg + id + T + cmd, mo + cmd)
  dropout: 0.1

  checkpoint_path: ~
  pretrain_path: ~
  load_strict: True